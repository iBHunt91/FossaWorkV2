# Intelligent Plan to Fix Batch Dispenser Scraper

## Executive Summary
The batch dispenser scraper is failing due to timing issues, overly complex verification logic, and differences from the working interactive script. This plan outlines a systematic approach to fix these issues.

## Root Cause Analysis

### 1. **Timing Issues**
- **Problem**: Page elements load asynchronously; scraper attempts extraction too early
- **Evidence**: Interactive script with proper waits works; batch scraper without waits fails
- **Impact**: 0 dispensers found even when data exists

### 2. **Complex Verification Logic**
- **Problem**: Using `wait_for_function` with complex JavaScript that may timeout
- **Evidence**: Logs show "No dispenser section found" despite dispenser text present
- **Impact**: False negatives in dispenser detection

### 3. **Inconsistent Selector Strategy**
- **Problem**: Multiple extraction methods with different selectors
- **Evidence**: Method 1, 2, and 3 all fail with different errors
- **Impact**: Confusion about which method should work

## Intelligent Fix Strategy

### Phase 1: Immediate Timing Fixes (Already Applied)
âœ… **Status**: Completed
- Added 3-second wait after navigation
- Increased Equipment tab wait to 3 seconds
- Increased modal check wait to 2 seconds

### Phase 2: Simplify Verification Logic
**Goal**: Replace complex JavaScript verification with simple, reliable checks

```python
# Instead of complex wait_for_function
# Use simple presence checks with retries
async def wait_for_equipment_content(page, max_retries=3):
    for i in range(max_retries):
        await page.wait_for_timeout(2000)
        
        # Simple check for content
        has_content = await page.evaluate("""
            () => {
                const body = document.body.textContent || '';
                return body.includes('Dispenser') && 
                       !body.includes('Loading') &&
                       !body.includes('Please wait');
            }
        """)
        
        if has_content:
            return True
            
        logger.debug(f"Retry {i+1}/{max_retries} - Content not ready yet")
    
    return True  # Continue anyway
```

### Phase 3: Unified Extraction Method
**Goal**: Consolidate multiple extraction methods into one reliable approach

```python
async def extract_dispensers_unified(page):
    """Single, reliable extraction method based on working patterns"""
    
    # Wait for content to stabilize
    await page.wait_for_timeout(1000)
    
    # Extract using the proven selector from interactive script
    dispensers = await page.evaluate("""
        () => {
            const containers = document.querySelectorAll('div.py-1\\\\.5, div.py-1\\\\.5.bg-gray-50');
            const results = [];
            
            containers.forEach(container => {
                // Extract dispenser info using consistent pattern
                const titleEl = container.querySelector('div.font-medium, div.font-semibold');
                if (!titleEl) return;
                
                const title = titleEl.textContent.trim();
                if (!title.includes('Dispenser')) return;
                
                // Extract all text content
                const allText = container.textContent;
                
                // Parse dispenser info
                const dispenser = {
                    title: title,
                    raw_text: allText,
                    // Extract specific fields using regex
                    serial_number: allText.match(/S\/N[:\s]+([A-Z0-9-]+)/i)?.[1],
                    make: allText.match(/Make[:\s]+([^,\n]+)/i)?.[1]?.trim(),
                    model: allText.match(/Model[:\s]+([^,\n]+)/i)?.[1]?.trim(),
                };
                
                results.push(dispenser);
            });
            
            return results;
        }
    """)
    
    return dispensers
```

### Phase 4: Intelligent Retry Mechanism
**Goal**: Add smart retries with exponential backoff

```python
async def scrape_with_retry(page, work_order_id, max_retries=3):
    """Intelligent retry with different strategies"""
    
    strategies = [
        {'wait_time': 3000, 'method': 'standard'},
        {'wait_time': 5000, 'method': 'extended_wait'},
        {'wait_time': 7000, 'method': 'force_refresh'}
    ]
    
    for i, strategy in enumerate(strategies[:max_retries]):
        try:
            logger.info(f"Attempt {i+1} using {strategy['method']} strategy")
            
            if strategy['method'] == 'force_refresh' and i > 0:
                # Refresh the page and try again
                await page.reload()
                await page.wait_for_timeout(strategy['wait_time'])
            
            # Navigate and scrape
            result = await scrape_dispensers_internal(page, work_order_id)
            
            if result and len(result) > 0:
                return result
                
        except Exception as e:
            logger.warning(f"Attempt {i+1} failed: {e}")
            
    return []  # All attempts failed
```

### Phase 5: Enhanced Debugging
**Goal**: Better visibility into failures

```python
async def debug_page_state(page, work_order_id):
    """Capture detailed page state for debugging"""
    
    debug_info = {
        'work_order_id': work_order_id,
        'url': page.url,
        'title': await page.title(),
        'timestamp': datetime.now().isoformat()
    }
    
    # Check for common issues
    debug_info['checks'] = {
        'has_equipment_tab': await page.query_selector('button:has-text("Equipment")') is not None,
        'has_dispenser_text': await page.evaluate("() => document.body.textContent.includes('Dispenser')"),
        'has_loading_indicator': await page.query_selector('.loading, .spinner') is not None,
        'has_error_message': await page.query_selector('.error, .alert-danger') is not None
    }
    
    # Save screenshot with annotations
    await page.screenshot(path=f'debug/dispenser_scrape_{work_order_id}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png')
    
    return debug_info
```

### Phase 6: Progressive Enhancement
**Goal**: Start simple, add complexity only if needed

1. **Level 1**: Basic scraping with proper waits
2. **Level 2**: Add retry mechanism if Level 1 fails
3. **Level 3**: Use alternative selectors if Level 2 fails
4. **Level 4**: Manual screenshot + OCR as last resort

## Implementation Plan

### Step 1: Create Test Suite (Day 1)
- [ ] Create comprehensive test cases with known dispenser counts
- [ ] Test against various page states (fast load, slow load, errors)
- [ ] Benchmark current vs improved performance

### Step 2: Implement Core Fixes (Day 1-2)
- [ ] Apply Phase 2 simplifications
- [ ] Implement Phase 3 unified extraction
- [ ] Add Phase 4 retry mechanism

### Step 3: Testing & Validation (Day 2-3)
- [ ] Run test suite against fixed implementation
- [ ] Compare results with interactive script
- [ ] Validate across different work order types

### Step 4: Monitoring & Optimization (Day 3-4)
- [ ] Add performance metrics
- [ ] Implement debug mode for production
- [ ] Create alerts for low success rates

### Step 5: Documentation & Deployment (Day 4-5)
- [ ] Update API documentation
- [ ] Create troubleshooting guide
- [ ] Deploy with feature flag for gradual rollout

## Success Metrics

1. **Reliability**: >95% success rate for dispenser extraction
2. **Performance**: <10 seconds per work order
3. **Accuracy**: 100% match with manual verification
4. **Resilience**: Handles network issues, slow pages, UI changes

## Risk Mitigation

1. **UI Changes**: Use multiple selector strategies
2. **Performance**: Implement concurrent scraping with limits
3. **Data Loss**: Cache successful extractions
4. **Debugging**: Comprehensive logging and screenshots

## Rollback Plan

If issues arise:
1. Revert to previous version via git
2. Use feature flag to disable batch scraping
3. Fall back to manual/interactive scraping
4. Notify users of temporary limitation

## Long-term Improvements

1. **Machine Learning**: Train model to identify dispensers in screenshots
2. **API Integration**: Request direct API access from WorkFossa
3. **Caching Strategy**: Store dispenser data with smart invalidation
4. **User Feedback**: Allow manual correction of extracted data

## Conclusion

This plan addresses the root causes systematically while providing fallback options and monitoring. The phased approach ensures we can validate improvements at each step without disrupting the existing system.