#!/usr/bin/env python3
"""
Test the visit URL extraction fix
"""
import asyncio
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.services.workfossa_scraper import WorkFossaScraper
from app.services.browser_automation import BrowserAutomationService
import logging

# Set up basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def test_visit_url_extraction():
    """Test that visit URLs are extracted correctly"""
    print("üß™ Testing Visit URL Extraction Fix")
    print("=" * 80)
    
    # Create browser automation service
    browser_service = BrowserAutomationService()
    
    # Create scraper instance
    scraper = WorkFossaScraper(browser_service)
    
    try:
        # Login
        print("\n1. Logging in to WorkFossa...")
        login_result = await scraper.login("ibhunt@fossaautomation.com", "7c!BJ8M2HQebAA$B")
        if not login_result['success']:
            print(f"‚ùå Login failed: {login_result.get('error', 'Unknown error')}")
            return
        print("‚úÖ Login successful")
        
        # Scrape work orders
        print("\n2. Scraping work orders...")
        work_orders = await scraper.scrape_work_orders(limit=5)
        
        if not work_orders:
            print("‚ùå No work orders found")
            return
            
        print(f"‚úÖ Found {len(work_orders)} work orders")
        
        # Analyze results
        print("\n3. Analyzing visit URLs:")
        print("-" * 80)
        
        correct_urls = 0
        incorrect_urls = 0
        null_urls = 0
        
        for wo in work_orders:
            visit_url = wo.visit_url
            visit_number = wo.visit_number
            
            print(f"\nWork Order: {wo.id}")
            print(f"  Site: {wo.site_name}")
            print(f"  Visit URL: {visit_url}")
            print(f"  Visit Number: {visit_number}")
            
            if visit_url:
                if '/visits/' in visit_url:
                    print("  ‚úÖ Correct format (contains /visits/)")
                    correct_urls += 1
                else:
                    print("  ‚ö†Ô∏è  Incorrect format (missing /visits/)")
                    incorrect_urls += 1
            else:
                print("  ‚ùå No visit URL")
                null_urls += 1
        
        print("\n" + "=" * 80)
        print("RESULTS:")
        print("=" * 80)
        print(f"‚úÖ Correct URLs: {correct_urls}")
        print(f"‚ö†Ô∏è  Incorrect URLs: {incorrect_urls}")
        print(f"‚ùå Null URLs: {null_urls}")
        
        if correct_urls > 0 and incorrect_urls == 0:
            print("\nüéâ SUCCESS! Visit URLs are now being extracted correctly!")
        elif correct_urls > 0:
            print("\n‚ö†Ô∏è  PARTIAL SUCCESS: Some visit URLs are correct, but some are still wrong")
        else:
            print("\n‚ùå FAILED: No correct visit URLs found")
            
    except Exception as e:
        print(f"\n‚ùå Error during test: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await browser_service.close()

if __name__ == "__main__":
    asyncio.run(test_visit_url_extraction())